{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3b472c3-4cc1-4381-972f-f2905dd755cc",
   "metadata": {},
   "source": [
    "# Build Your First AI Agent with LangGraph\n",
    "\n",
    "Welcome to this hands-on workshop!\n",
    "\n",
    "## Context  \n",
    "\n",
    "Large Language Models (LLMs) are powerful, but on their own they can feel limited, often just responding to prompts without much structure. Agents, on the other hand, allow LLMs to **reason, decide, and act** using tools, making them much more practical for real-world tasks.  \n",
    "\n",
    "The challenge is that building agents isn’t always straightforward. You often need more **control and precision**: maybe your agent should always check the web before answering certain questions, or behave differently depending on the situation.  \n",
    "\n",
    "This is where [LangGraph](https://langchain-ai.github.io/langgraph/) comes in. LangGraph is a framework for building **agents and multi-agent applications**, giving you more control over workflows, making them reliable and production-ready.  \n",
    "\n",
    "In this workshop, we’ll be building a simple **ReACT Agent** that:  \n",
    "- Uses an LLM for general conversation and creative tasks  \n",
    "- Calls a search tool when it needs real-time information  \n",
    "- Decides the right path (LLM or search) based on your question  \n",
    "\n",
    "By the end, you’ll have a working agent that can **chat and fetch real-world answers**.  \n",
    "\n",
    "## Workshop Structure  \n",
    "\n",
    "This workshop is organized into a set of Jupyter notebooks. Each notebook builds on the previous one:  \n",
    "1. **Setup** – Installing dependencies, Load environment variables, and test API keys\n",
    "2. **Intro to LangGraph** – What is LangGraph and it's key concepts\n",
    "3. **Building Blocks** – Define nodes, add edges and state\n",
    "4. **Simple ReAct Agent** – Use an LLM as a node, add a tool, and wrap into an agent graph\n",
    "5. **Research Agent** – Running real examples  \n",
    "\n",
    "You can run the notebooks directly, follow along with the code, or explore them afterward at your own pace.  \n",
    "\n",
    "## Setup  \n",
    "\n",
    "Before you begin, please follow the instructions in the `README` to create an environment and install dependencies.\n",
    "\n",
    "## Chat models\n",
    "\n",
    "In this course, we'll be using [Chat Models](https://python.langchain.com/v0.2/docs/concepts/#chat-models), which do a few things take a sequence of messages as inputs and return chat messages as outputs. LangChain does not host any Chat Models, rather we rely on third party integrations. [Here](https://python.langchain.com/v0.2/docs/integrations/chat/) is a list of 3rd party chat model integrations within LangChain! By default, the course will use [ChatGroq](https://python.langchain.com/docs/integrations/chat/groq/) and [ChatGoogleGenerativeAI](https://python.langchain.com/docs/integrations/chat/google_generative_ai/) as they provide free APIs for LLMs. As noted, please ensure that you have an `GROQ_API_KEY` and `GOOGLE_API_KEY`.\n",
    "\n",
    "Let's check that your `OPENAI_API_KEY` and `GOOGLE_API_KEY` is set and, if not, you will be asked to enter it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda1656d-afb9-4dab-8149-3253f6dc532c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langchain_groq langchain-google-genai langchain_core langchain_community tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd17addd-ca86-4102-b14f-ee598d24d4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ba1f3f-2f29-445d-9fb4-3c86df3cc379",
   "metadata": {},
   "source": [
    "If you do not have your Groq API key yet, you may obtain one [here](https://console.groq.com/keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89e22fc-7ba6-46c5-91bb-56c48ef47411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00a14c2-b1b4-4e5e-b411-cc4e32047828",
   "metadata": {},
   "source": [
    "If you do not have your Gemini API key yet, you may obtain one [here](https://aistudio.google.com/app/apikey).\n",
    "\n",
    "If your school or organization restricts access to Google Gemini Studio, try using a personal Google account to generate your API key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0446d938-43f0-461d-9424-8b8e166f9b40",
   "metadata": {},
   "source": [
    "[Here](https://python.langchain.com/v0.2/docs/how_to/#chat-models) is a useful how-to for everything you can do with chat models—we’ll highlight a few key points below.  \n",
    "\n",
    "If you’ve run `pip install -r requirements.txt` as noted in the README, you’ve installed the `langchain-groq` and `langchain-google-genai` packages. With these, you can instantiate both `ChatGroq` and `ChatGoogleGenerativeAI` model objects.  \n",
    "\n",
    "- You can see the **rate limits** for Groq models [here](https://console.groq.com/docs/rate-limits) and Google Gemini models [here](https://ai.google.dev/gemini-api/docs/rate-limits).  \n",
    "- By default, the notebooks use **`llama-3.3-70b-versatile`**, which is optimized for a wide range of natural language processing tasks, delivers strong benchmark performance, and maintains efficiency across diverse applications ([see more here](https://console.groq.com/docs/model/llama-3.3-70b-versatile)).  \n",
    "\n",
    "There are [a few standard parameters](https://python.langchain.com/v0.2/docs/concepts/#chat-models) you can set with chat models. Two of the most common are:  \n",
    "\n",
    "* `model`: the name of the model  \n",
    "* `temperature`: the sampling temperature  \n",
    "\n",
    "The `temperature` parameter controls the randomness or creativity of the model’s output:  \n",
    "\n",
    "- **Low temperature (close to 0)** → deterministic, focused, and factual (best for accuracy-critical tasks)  \n",
    "- **High temperature (close to 1)** → more diverse, creative, and exploratory (best for brainstorming or open-ended tasks)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03487950-5c85-4c8e-a34b-6e0b03c0bbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llama_chat = ChatGroq(model=\"llama-3.3-70b-versatile\", temperature=0)\n",
    "gemini_chat = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2868f92-dc35-4d58-a986-56462f38136a",
   "metadata": {},
   "source": [
    "Chat models in LangChain have a number of [default methods](https://python.langchain.com/v0.2/docs/concepts/#runnable-interface). For the most part, we'll be using:\n",
    "\n",
    "* `stream`: provides the \"answer in progress,\" allowing for real-time interaction and feedback.\n",
    "* `invoke`: provides the \"final answer\" all at once.\n",
    "\n",
    "And, as mentioned, chat models take [messages](https://python.langchain.com/v0.2/docs/concepts/#messages) as input. Messages have a role (that describes who is saying the message) and a content property. We'll be talking a lot more about this later, but here let's just show the basics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40cc23b-dc5c-441e-a835-b289928d0387",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Create a message\n",
    "msg = HumanMessage(content=\"Hello world\", name=\"Lance\")\n",
    "\n",
    "# Message list\n",
    "messages = [msg]\n",
    "\n",
    "# Invoke the model with a list of messages \n",
    "llama_chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b94fd7e-3271-46c0-906b-b1b665bc89d4",
   "metadata": {},
   "source": [
    "We get an `AIMessage` response. Also, note that we can just invoke a chat model with a string. When a string is passed in as input, it is converted to a `HumanMessage` and then passed to the underlying model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d1b982-c4af-41fb-81c1-6c17c8990d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_chat.invoke(\"hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e652f7dd-600c-4f3f-8794-13faded1115b",
   "metadata": {},
   "source": [
    "The interface is consistent across all chat models and models are typically initialized once at the start up each notebooks. \n",
    "\n",
    "So, you can easily switch between models without changing the downstream code if you have strong preference for another provider."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57a9487-725e-43af-a9b8-7adebbf4f6b1",
   "metadata": {},
   "source": [
    "## Search Tools\n",
    "\n",
    "You'll also see [Tavily](https://tavily.com/) in the README, which is a search engine optimized for LLMs and RAG, aimed at efficient, quick, and persistent search results. As mentioned, it's easy to sign up and offers a generous free tier. We will use Tavily by default but, of course, other search tools can be used if you want to modify the code for yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd348b5c-1150-45c0-aa2f-70bf1f9d55d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "_set_env(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bc90a3-bc0d-46e1-8a0a-500e58ce38da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "tavily_search = TavilySearch(max_results=3)\n",
    "search_docs = tavily_search.invoke(\"What is LangGraph?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acf7095-fcb0-4010-ab2e-73a212780a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f910271-0c01-44c1-a855-cb6c0625c79b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
