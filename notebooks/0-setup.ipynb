{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3b472c3-4cc1-4381-972f-f2905dd755cc",
   "metadata": {},
   "source": [
    "# Build Your First AI Agent with LangGraph\n",
    "\n",
    "Welcome to this hands-on workshop!\n",
    "\n",
    "## Context  \n",
    "\n",
    "Large Language Models (LLMs) are powerful, but on their own they can feel limited, often just responding to prompts without much structure. Agents, on the other hand, allow LLMs to **reason, decide, and act** using tools, making them much more practical for real-world tasks.  \n",
    "\n",
    "The challenge is that building agents isn’t always straightforward. You often need more **control and precision**: maybe your agent should always check the web before answering certain questions, or behave differently depending on the situation.  \n",
    "\n",
    "This is where [LangGraph](https://langchain-ai.github.io/langgraph/) comes in. LangGraph is a framework for building **agents and multi-agent applications**, giving you more control over workflows, making them reliable and production-ready.  \n",
    "\n",
    "In this workshop, we’ll be building a simple **ReACT Agent** that:  \n",
    "- Uses an LLM for general conversation and creative tasks  \n",
    "- Calls a search tool when it needs real-time information  \n",
    "- Decides the right path (LLM or search) based on your question  \n",
    "\n",
    "By the end, you’ll have a working agent that can **chat and fetch real-world answers**.  \n",
    "\n",
    "## Workshop Structure  \n",
    "\n",
    "This workshop is organized into a set of Jupyter notebooks. Each notebook builds on the previous one:  \n",
    "1. **Setup** – Installing dependencies, Load environment variables, and test API keys\n",
    "2. **Intro to LangGraph** – What is LangGraph and it's key concepts\n",
    "3. **Building Blocks** – Define nodes, add edges and state\n",
    "4. **Simple ReAct Agent** – Use an LLM as a node, add a tool, and wrap into an agent graph\n",
    "5. **Research Agent** – Running real examples  \n",
    "\n",
    "You can run the notebooks directly, follow along with the code, or explore them afterward at your own pace.  \n",
    "\n",
    "## Setup  \n",
    "\n",
    "Before you begin, please follow the instructions in the `README` to create an environment and install dependencies.\n",
    "\n",
    "## Chat models\n",
    "\n",
    "In this course, we'll be using [Chat Models](https://python.langchain.com/v0.2/docs/concepts/#chat-models), which do a few things take a sequence of messages as inputs and return chat messages as outputs. LangChain does not host any Chat Models, rather we rely on third party integrations. [Here](https://python.langchain.com/v0.2/docs/integrations/chat/) is a list of 3rd party chat model integrations within LangChain! By default, the course will use [ChatGroq](https://python.langchain.com/docs/integrations/chat/groq/) and [ChatGoogleGenerativeAI](https://python.langchain.com/docs/integrations/chat/google_generative_ai/) as they provide free APIs for LLMs. As noted, please ensure that you have an `GROQ_API_KEY` and `GOOGLE_API_KEY`.\n",
    "\n",
    "Let's check that your `OPENAI_API_KEY` and `GOOGLE_API_KEY` is set and, if not, you will be asked to enter it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cda1656d-afb9-4dab-8149-3253f6dc532c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langchain_groq langchain-google-genai langchain_core langchain_community langchain-tavily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d784ff4c-2fe5-47fb-b64d-4a8b37e2153f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd17addd-ca86-4102-b14f-ee598d24d4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ba1f3f-2f29-445d-9fb4-3c86df3cc379",
   "metadata": {},
   "source": [
    "If you do not have your Groq API key yet, you may obtain one [here](https://console.groq.com/keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f89e22fc-7ba6-46c5-91bb-56c48ef47411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00a14c2-b1b4-4e5e-b411-cc4e32047828",
   "metadata": {},
   "source": [
    "If you do not have your Gemini API key yet, you may obtain one [here](https://aistudio.google.com/app/apikey).\n",
    "\n",
    "If your school or organization restricts access to Google Gemini Studio, try using a personal Google account to generate your API key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0446d938-43f0-461d-9424-8b8e166f9b40",
   "metadata": {},
   "source": [
    "[Here](https://python.langchain.com/v0.2/docs/how_to/#chat-models) is a useful how-to for everything you can do with chat models—we’ll highlight a few key points below.  \n",
    "\n",
    "If you’ve run `pip install -r requirements.txt` as noted in the README, you’ve installed the `langchain-groq` and `langchain-google-genai` packages. With these, you can instantiate both `ChatGroq` and `ChatGoogleGenerativeAI` model objects.  \n",
    "\n",
    "- You can see the **rate limits** for Groq models [here](https://console.groq.com/docs/rate-limits) and Google Gemini models [here](https://ai.google.dev/gemini-api/docs/rate-limits).  \n",
    "- By default, the notebooks use **`llama-3.3-70b-versatile`**, which is optimized for a wide range of natural language processing tasks, delivers strong benchmark performance, and maintains efficiency across diverse applications ([see more here](https://console.groq.com/docs/model/llama-3.3-70b-versatile)).  \n",
    "\n",
    "There are [a few standard parameters](https://python.langchain.com/v0.2/docs/concepts/#chat-models) you can set with chat models. Two of the most common are:  \n",
    "\n",
    "* `model`: the name of the model  \n",
    "* `temperature`: the sampling temperature  \n",
    "\n",
    "The `temperature` parameter controls the randomness or creativity of the model’s output:  \n",
    "\n",
    "- **Low temperature (close to 0)** → deterministic, focused, and factual (best for accuracy-critical tasks)  \n",
    "- **High temperature (close to 1)** → more diverse, creative, and exploratory (best for brainstorming or open-ended tasks)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03487950-5c85-4c8e-a34b-6e0b03c0bbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llama_chat = ChatGroq(model=\"llama-3.3-70b-versatile\", temperature=0)\n",
    "gemini_chat = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2868f92-dc35-4d58-a986-56462f38136a",
   "metadata": {},
   "source": [
    "Chat models in LangChain have a number of [default methods](https://python.langchain.com/v0.2/docs/concepts/#runnable-interface). For the most part, we'll be using:\n",
    "\n",
    "* `stream`: provides the \"answer in progress,\" allowing for real-time interaction and feedback.\n",
    "* `invoke`: provides the \"final answer\" all at once.\n",
    "\n",
    "And, as mentioned, chat models take [messages](https://python.langchain.com/v0.2/docs/concepts/#messages) as input. Messages have a role (that describes who is saying the message) and a content property. We'll be talking a lot more about this later, but here let's just show the basics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a40cc23b-dc5c-441e-a835-b289928d0387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello World. It's nice to meet you. Is there something I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 37, 'total_tokens': 63, 'completion_time': 0.06605359, 'prompt_time': 0.010286471, 'queue_time': 0.045461209, 'total_time': 0.076340061}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_2ddfbb0da0', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--f2cb492b-55f1-43ba-9bfa-c0d5981e1000-0', usage_metadata={'input_tokens': 37, 'output_tokens': 26, 'total_tokens': 63})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Create a message\n",
    "msg = HumanMessage(content=\"Hello World\", name=\"Naza\")\n",
    "\n",
    "# Message list\n",
    "messages = [msg]\n",
    "\n",
    "# Invoke the model with a list of messages \n",
    "llama_chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b94fd7e-3271-46c0-906b-b1b665bc89d4",
   "metadata": {},
   "source": [
    "We get an `AIMessage` response. Also, note that we can just invoke a chat model with a string. When a string is passed in as input, it is converted to a `HumanMessage` and then passed to the underlying model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43d1b982-c4af-41fb-81c1-6c17c8990d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello World!\\n\\nIt's great to hear from you. How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--b9f64c51-fe99-4b49-9bae-26ce06d07e7b-0', usage_metadata={'input_tokens': 3, 'output_tokens': 52, 'total_tokens': 55, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 32}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini_chat.invoke(\"Hello World\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e652f7dd-600c-4f3f-8794-13faded1115b",
   "metadata": {},
   "source": [
    "The interface is consistent across all chat models and models are typically initialized once at the start up each notebooks. \n",
    "\n",
    "So, you can easily switch between models without changing the downstream code if you have strong preference for another provider."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57a9487-725e-43af-a9b8-7adebbf4f6b1",
   "metadata": {},
   "source": [
    "## Search Tools\n",
    "\n",
    "You'll also see [Tavily](https://tavily.com/) in the README, which is a search engine optimized for LLMs and RAG, aimed at efficient, quick, and persistent search results. As mentioned, it's easy to sign up and offers a generous free tier. We will use Tavily by default but, of course, other search tools can be used if you want to modify the code for yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd348b5c-1150-45c0-aa2f-70bf1f9d55d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "        \n",
    "_set_env(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85bc90a3-bc0d-46e1-8a0a-500e58ce38da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "tavily_search = TavilySearch(max_results=3)\n",
    "search_docs = tavily_search.invoke(\"What is LangGraph?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0acf7095-fcb0-4010-ab2e-73a212780a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is LangGraph?',\n",
       " 'follow_up_questions': None,\n",
       " 'answer': None,\n",
       " 'images': [],\n",
       " 'results': [{'url': 'https://www.ibm.com/think/topics/langgraph',\n",
       "   'title': 'What is LangGraph?',\n",
       "   'content': 'LangGraph, created by LangChain, is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows. At its core, LangGraph uses the power of graph-based architectures to model and manage the intricate relationships between various components of an AI agent workflow. LangGraph illuminates the processes within an AI workflow, allowing full transparency of the agent’s state. By combining these technologies with a set of APIs and tools, LangGraph provides users with a versatile platform for developing AI solutions and workflows including chatbots, state graphs and other agent-based systems. **Nodes**: In LangGraph, nodes represent individual components or agents within an AI workflow. LangGraph uses enhanced decision-making by modeling complex relationships between nodes, which means it uses AI agents to analyze their past actions and feedback.',\n",
       "   'score': 0.9635647,\n",
       "   'raw_content': None},\n",
       "  {'url': 'https://www.datacamp.com/tutorial/langgraph-tutorial',\n",
       "   'title': 'LangGraph Tutorial: What Is LangGraph and How to Use It?',\n",
       "   'content': 'LangGraph is a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner. By managing the flow of data and the sequence of operations, LangGraph allows developers to focus on the high-level logic of their applications rather than the intricacies of agent coordination. Whether you need a chatbot that can handle various types of user requests or a multi-agent system that performs complex tasks, LangGraph provides the tools to build exactly what you need. LangGraph significantly simplifies the development of complex LLM applications by providing a structured framework for managing state and coordinating agent interactions.',\n",
       "   'score': 0.9581988,\n",
       "   'raw_content': None},\n",
       "  {'url': 'https://www.analyticsvidhya.com/blog/2024/07/langgraph-revolutionizing-ai-agent/',\n",
       "   'title': 'What is LangGraph?',\n",
       "   'content': '* LangGraph is a library built on top of Langchain that is designed to facilitate the creation of cyclic graphs for large language model (LLM) – based AI agents. * Langgraph introduces a chat agent executor that represents the agent state as a list of messages, which is particularly useful for newer, chat-based models. The agent executor class in the Langchain framework was the main tool for building and executing AI agents before LangGraph. Large Language Models (LLMs) are the foundation for designing sophisticated AI agents, and LangGraph, built on top of Langchain, is intended to make the process of creating cyclic graphs easier. Ans. LangGraph addresses the limitations of previous AI agent development frameworks by providing more flexibility, better state management, and support for cyclic execution and multi-agent systems.',\n",
       "   'score': 0.95132804,\n",
       "   'raw_content': None}],\n",
       " 'response_time': 0.77,\n",
       " 'request_id': 'b7132e32-6e17-4c37-90af-efe4bfa716e9'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f910271-0c01-44c1-a855-cb6c0625c79b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
