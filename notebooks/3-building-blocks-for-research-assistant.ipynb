{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "626a3eba-7872-4eb4-ae6c-7d59b2d2591f",
   "metadata": {},
   "source": [
    "# Parallel node execution\n",
    "\n",
    "## Goals\n",
    "\n",
    "We will dive into `multi-agent` workflows, and build up to a multi-agent research assistant that ties together all of the modules from this course.\n",
    "\n",
    "To build this multi-agent research assistant, we'll first discuss a few LangGraph controllability topics.\n",
    "\n",
    "We'll start with [parallelization](https://langchain-ai.github.io/langgraph/how-tos/branching/#how-to-create-branches-for-parallel-node-execution).\n",
    "\n",
    "## Fan out and fan in\n",
    "\n",
    "Let's build a simple linear graph that over-writes the state at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e47820-eca4-40e1-91ac-5a943df463d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U  langgraph langchain-tavily wikipedia langchain_groq langchain_community langgraph_sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce1e734-ea19-4ece-a04d-8aa1432fb30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1864a6-e110-4232-870e-4e3002f5f73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "from typing import Any\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "class State(TypedDict):\n",
    "    # The operator.add reducer fn makes this append-only\n",
    "    state: str\n",
    "\n",
    "class ReturnNodeValue:\n",
    "    def __init__(self, node_secret: str):\n",
    "        self._value = node_secret\n",
    "\n",
    "    def __call__(self, state: State) -> Any:\n",
    "        print(f\"Adding {self._value} to {state['state']}\")\n",
    "        return {\"state\": [self._value]}\n",
    "\n",
    "# Add nodes\n",
    "builder = StateGraph(State)\n",
    "\n",
    "# Initialize each node with node_secret \n",
    "builder.add_node(\"a\", ReturnNodeValue(\"I'm A\"))\n",
    "builder.add_node(\"b\", ReturnNodeValue(\"I'm B\"))\n",
    "builder.add_node(\"c\", ReturnNodeValue(\"I'm C\"))\n",
    "builder.add_node(\"d\", ReturnNodeValue(\"I'm D\"))\n",
    "\n",
    "# Flow\n",
    "builder.add_edge(START, \"a\")\n",
    "builder.add_edge(\"a\", \"b\")\n",
    "builder.add_edge(\"b\", \"c\")\n",
    "builder.add_edge(\"c\", \"d\")\n",
    "builder.add_edge(\"d\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcf168e-74fb-4be8-a38b-5e86456a6022",
   "metadata": {},
   "source": [
    "We over-write state, as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f20a8e7-a8f8-48ce-ad0f-6d4d33b38444",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke({\"state\": []})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e150af5-adec-4558-8c54-15b730294b8f",
   "metadata": {},
   "source": [
    "Now, let's run `b` and `c` in parallel. \n",
    "\n",
    "And then run `d`.\n",
    "\n",
    "We can do this easily with fan-out from `a` to `b` and `c`, and then fan-in to `d`.\n",
    "\n",
    "The the state updates are applied at the end of each step.\n",
    "\n",
    "Let's run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15712a60-5dc9-4699-b70d-39a7606a7e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(State)\n",
    "\n",
    "# Initialize each node with node_secret \n",
    "builder.add_node(\"a\", ReturnNodeValue(\"I'm A\"))\n",
    "builder.add_node(\"b\", ReturnNodeValue(\"I'm B\"))\n",
    "builder.add_node(\"c\", ReturnNodeValue(\"I'm C\"))\n",
    "builder.add_node(\"d\", ReturnNodeValue(\"I'm D\"))\n",
    "\n",
    "# Flow\n",
    "builder.add_edge(START, \"a\")\n",
    "builder.add_edge(\"a\", \"b\")\n",
    "builder.add_edge(\"a\", \"c\")\n",
    "builder.add_edge(\"b\", \"d\")\n",
    "builder.add_edge(\"c\", \"d\")\n",
    "builder.add_edge(\"d\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae69d7c-a98e-41c6-805b-ab246b3a3479",
   "metadata": {},
   "source": [
    "**We see an error**! \n",
    "\n",
    "This is because both `b` and `c` are writing to the same state key / channel in the same step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b100fa6-e3d9-47e2-b100-931e98fbd6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.errors import InvalidUpdateError\n",
    "try:\n",
    "    graph.invoke({\"state\": []})\n",
    "except InvalidUpdateError as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c3aa46-c717-4917-97fe-2868a12d5142",
   "metadata": {},
   "source": [
    "When using fan out, we need to be sure that we are using a reducer if steps are writing to the same the channel / key. \n",
    "\n",
    "`operator.add` is a function from Python's built-in operator module.\n",
    "\n",
    "When `operator.add` is applied to lists, it performs list concatenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5800f913-79be-4b91-9421-c7a959fe583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated\n",
    "\n",
    "class State(TypedDict):\n",
    "    # The operator.add reducer fn makes this append-only\n",
    "    state: Annotated[list, operator.add]\n",
    "\n",
    "# Add nodes\n",
    "builder = StateGraph(State)\n",
    "\n",
    "# Initialize each node with node_secret \n",
    "builder.add_node(\"a\", ReturnNodeValue(\"I'm A\"))\n",
    "builder.add_node(\"b\", ReturnNodeValue(\"I'm B\"))\n",
    "builder.add_node(\"c\", ReturnNodeValue(\"I'm C\"))\n",
    "builder.add_node(\"d\", ReturnNodeValue(\"I'm D\"))\n",
    "\n",
    "# Flow\n",
    "builder.add_edge(START, \"a\")\n",
    "builder.add_edge(\"a\", \"b\")\n",
    "builder.add_edge(\"a\", \"c\")\n",
    "builder.add_edge(\"b\", \"d\")\n",
    "builder.add_edge(\"c\", \"d\")\n",
    "builder.add_edge(\"d\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44056c69-2eaf-43d4-8413-76304219c4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke({\"state\": []})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b728d87e-9e17-4ac5-a99f-c666e4e361a6",
   "metadata": {},
   "source": [
    "Now we see that we append to state for the updates made in parallel by `b` and `c`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f3f8d4-0cce-4b48-acf2-c611b5a0fac2",
   "metadata": {},
   "source": [
    "## Waiting for nodes to finish\n",
    "\n",
    "Now, lets consider a case where one parallel path has more steps than the other one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b4179b-06dd-409c-baaf-30ed1817e215",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(State)\n",
    "\n",
    "# Initialize each node with node_secret \n",
    "builder.add_node(\"a\", ReturnNodeValue(\"I'm A\"))\n",
    "builder.add_node(\"b\", ReturnNodeValue(\"I'm B\"))\n",
    "builder.add_node(\"b2\", ReturnNodeValue(\"I'm B2\"))\n",
    "builder.add_node(\"c\", ReturnNodeValue(\"I'm C\"))\n",
    "builder.add_node(\"d\", ReturnNodeValue(\"I'm D\"))\n",
    "\n",
    "# Flow\n",
    "builder.add_edge(START, \"a\")\n",
    "builder.add_edge(\"a\", \"b\")\n",
    "builder.add_edge(\"a\", \"c\")\n",
    "builder.add_edge(\"b\", \"b2\")\n",
    "builder.add_edge([\"b2\", \"c\"], \"d\")\n",
    "builder.add_edge(\"d\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c68070-2125-4676-954f-7f3e3438d724",
   "metadata": {},
   "source": [
    "In this case, `b`, `b2`, and `c` are all part of the same step.\n",
    "\n",
    "The graph will wait for all of these to be completed before proceeding to step `d`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3200eff8-ba7e-4714-87a0-a22de78fb9c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mgraph\u001b[49m.invoke({\u001b[33m\"\u001b[39m\u001b[33mstate\u001b[39m\u001b[33m\"\u001b[39m: []})\n",
      "\u001b[31mNameError\u001b[39m: name 'graph' is not defined"
     ]
    }
   ],
   "source": [
    "graph.invoke({\"state\": []})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6e7f55-bbb1-4205-9959-5ecf966d32f3",
   "metadata": {},
   "source": [
    "## Setting the order of state updates\n",
    "\n",
    "However, within each step we don't have specific control over the order of the state updates!\n",
    "\n",
    "In simple terms, it is a deterministic order determined by LangGraph based upon graph topology that **we do not control**. \n",
    "\n",
    "Above, we see that `c` is added before `b2`.\n",
    "\n",
    "However, we can use a custom reducer to customize this e.g., sort state updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce48a544-e019-479c-b235-3593020a2a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorting_reducer(left, right):\n",
    "    \"\"\" Combines and sorts the values in a list\"\"\"\n",
    "    if not isinstance(left, list):\n",
    "        left = [left]\n",
    "\n",
    "    if not isinstance(right, list):\n",
    "        right = [right]\n",
    "    \n",
    "    return sorted(left + right, reverse=False)\n",
    "\n",
    "class State(TypedDict):\n",
    "    # sorting_reducer will sort the values in state\n",
    "    state: Annotated[list, sorting_reducer]\n",
    "\n",
    "# Add nodes\n",
    "builder = StateGraph(State)\n",
    "\n",
    "# Initialize each node with node_secret \n",
    "builder.add_node(\"a\", ReturnNodeValue(\"I'm A\"))\n",
    "builder.add_node(\"b\", ReturnNodeValue(\"I'm B\"))\n",
    "builder.add_node(\"b2\", ReturnNodeValue(\"I'm B2\"))\n",
    "builder.add_node(\"c\", ReturnNodeValue(\"I'm C\"))\n",
    "builder.add_node(\"d\", ReturnNodeValue(\"I'm D\"))\n",
    "\n",
    "# Flow\n",
    "builder.add_edge(START, \"a\")\n",
    "builder.add_edge(\"a\", \"b\")\n",
    "builder.add_edge(\"a\", \"c\")\n",
    "builder.add_edge(\"b\", \"b2\")\n",
    "builder.add_edge([\"b2\", \"c\"], \"d\")\n",
    "builder.add_edge(\"d\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45204dd9-bfca-44ab-990d-9081bf2ab2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke({\"state\": []})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bf6ee1-b3c5-44aa-a65e-720f74087335",
   "metadata": {},
   "source": [
    "Now, the reducer sorts the updated state values!\n",
    "\n",
    "The `sorting_reducer` example sorts all values globally. We can also: \n",
    "\n",
    "1. Write outputs to a separate field in the state during the parallel step\n",
    "2. Use a \"sink\" node after the parallel step to combine and order those outputs\n",
    "3. Clear the temporary field after combining\n",
    "\n",
    "See the [docs](https://langchain-ai.github.io/langgraph/how-tos/branching/#stable-sorting) for more details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9fff55-c7d4-4a93-aafc-963a5d5c4e48",
   "metadata": {},
   "source": [
    "## Working with LLMs\n",
    "\n",
    "Now, lets add a realistic example! \n",
    "\n",
    "We want to gather context from two external sources (Wikipedia and Web-Search) and have an LLM answer a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53cbd38-ffbe-4383-a61f-3b5d0d01ab55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"llama-3.3-70b-versatile\", temperature=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1c87e2-5878-4ba4-bfe3-46822d153003",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    question: str\n",
    "    answer: str\n",
    "    context: Annotated[list, operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c86746-c758-4cc0-a84f-a0ec1952d3ae",
   "metadata": {},
   "source": [
    "You can try different web search tools. [Tavily](https://tavily.com/) is one nice option to consider, but ensure your `TAVILY_API_KEY` is set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2221078-15e6-4b96-a8ed-8b27408cb5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "_set_env(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6b5e8c-2137-445f-aada-2b1b22f70ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "def search_web(state):\n",
    "    \n",
    "    \"\"\" Retrieve docs from web search \"\"\"\n",
    "\n",
    "    # Search\n",
    "    tavily_search = TavilySearchResults(max_results=3)\n",
    "    search_docs = tavily_search.invoke(state['question'])\n",
    "\n",
    "    # Format\n",
    "    formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
    "        [\n",
    "            f'<Document href=\"{doc[\"url\"]}\">\\n{doc[\"content\"]}\\n</Document>'\n",
    "            for doc in search_docs\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\"context\": [formatted_search_docs]} \n",
    "\n",
    "def search_wikipedia(state):\n",
    "    \n",
    "    \"\"\" Retrieve docs from wikipedia \"\"\"\n",
    "\n",
    "    # Search\n",
    "    search_docs = WikipediaLoader(query=state['question'], \n",
    "                                  load_max_docs=2).load()\n",
    "\n",
    "     # Format\n",
    "    formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
    "        [\n",
    "            f'<Document source=\"{doc.metadata[\"source\"]}\" page=\"{doc.metadata.get(\"page\", \"\")}\">\\n{doc.page_content}\\n</Document>'\n",
    "            for doc in search_docs\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\"context\": [formatted_search_docs]} \n",
    "\n",
    "def generate_answer(state):\n",
    "    \n",
    "    \"\"\" Node to answer a question \"\"\"\n",
    "\n",
    "    # Get state\n",
    "    context = state[\"context\"]\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Template\n",
    "    answer_template = \"\"\"Answer the question {question} using this context: {context}\"\"\"\n",
    "    answer_instructions = answer_template.format(question=question, \n",
    "                                                       context=context)    \n",
    "    \n",
    "    # Answer\n",
    "    answer = llm.invoke([SystemMessage(content=answer_instructions)]+[HumanMessage(content=f\"Answer the question.\")])\n",
    "      \n",
    "    # Append it to state\n",
    "    return {\"answer\": answer}\n",
    "\n",
    "# Add nodes\n",
    "builder = StateGraph(State)\n",
    "\n",
    "# Initialize each node with node_secret \n",
    "builder.add_node(\"search_web\",search_web)\n",
    "builder.add_node(\"search_wikipedia\", search_wikipedia)\n",
    "builder.add_node(\"generate_answer\", generate_answer)\n",
    "\n",
    "# Flow\n",
    "builder.add_edge(START, \"search_wikipedia\")\n",
    "builder.add_edge(START, \"search_web\")\n",
    "builder.add_edge(\"search_wikipedia\", \"generate_answer\")\n",
    "builder.add_edge(\"search_web\", \"generate_answer\")\n",
    "builder.add_edge(\"generate_answer\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95f7af1-06d3-4e58-8e11-65f2e08b69e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph.invoke({\"question\": \"Impact of artificial intelligence on education\"})\n",
    "result['answer'].content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3765634-8e33-484a-912e-49cc7049658c",
   "metadata": {},
   "source": [
    "# Sub-graphs\n",
    "\n",
    "## Review\n",
    "\n",
    "We're building up to a multi-agent research assistant that ties together everything we knew so far.\n",
    "\n",
    "We just covered parallelization, which is one important LangGraph controllability topic.\n",
    "\n",
    "## Goals\n",
    "\n",
    "Now, we're [going to cover sub-graphs](https://langchain-ai.github.io/langgraph/how-tos/subgraph/#simple-example).\n",
    "\n",
    "## State\n",
    "\n",
    "Sub-graphs allow you to create and manage different states in different parts of your graph. \n",
    "\n",
    "This is particularly useful for multi-agent systems, with teams of agents that each have their own state.\n",
    "\n",
    "Let's consider a toy example:\n",
    "\n",
    "* I have a system that accepts logs\n",
    "* It performs two separate sub-tasks by different agents (summarize logs, find failure modes)\n",
    "* I want to perform these two operations in two different sub-graphs.\n",
    "\n",
    "The most critical thing to understand is how the graphs communicate! \n",
    "\n",
    "In short, communication is **done with over-lapping keys**: \n",
    "\n",
    "* The sub-graphs can access `docs` from the parent\n",
    "* The parent can access `summary/failure_report` from the sub-graphs\n",
    "\n",
    "![subgraph.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbb1abf89f2d847ee6f1ff_sub-graph1.png)\n",
    "\n",
    "## Input\n",
    "\n",
    "Let's define a schema for the logs that will be input to our graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8be98b6-6db4-40c1-b9e6-6eacf371bb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langchain-academy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc85d12-6b69-4594-aff4-5fe3d9e64bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "from typing_extensions import TypedDict\n",
    "from typing import List, Optional, Annotated\n",
    "\n",
    "# The structure of the logs\n",
    "class Log(TypedDict):\n",
    "    id: str\n",
    "    question: str\n",
    "    docs: Optional[List]\n",
    "    answer: str\n",
    "    grade: Optional[int]\n",
    "    grader: Optional[str]\n",
    "    feedback: Optional[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df150ca7-7eb0-4fd6-957e-9e255fa88318",
   "metadata": {},
   "source": [
    "## Sub graphs\n",
    "\n",
    "Here is the failure analysis sub-graph, which uses `FailureAnalysisState`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f17bd03-af05-4117-9a5c-d19089d81fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Failure Analysis Sub-graph\n",
    "class FailureAnalysisState(TypedDict):\n",
    "    cleaned_logs: List[Log]\n",
    "    failures: List[Log]\n",
    "    fa_summary: str\n",
    "    processed_logs: List[str]\n",
    "\n",
    "class FailureAnalysisOutputState(TypedDict):\n",
    "    fa_summary: str\n",
    "    processed_logs: List[str]\n",
    "\n",
    "def get_failures(state):\n",
    "    \"\"\" Get logs that contain a failure \"\"\"\n",
    "    cleaned_logs = state[\"cleaned_logs\"]\n",
    "    failures = [log for log in cleaned_logs if \"grade\" in log]\n",
    "    return {\"failures\": failures}\n",
    "\n",
    "def generate_summary(state):\n",
    "    \"\"\" Generate summary of failures \"\"\"\n",
    "    failures = state[\"failures\"]\n",
    "    # Add fxn: fa_summary = summarize(failures)\n",
    "    fa_summary = \"Poor quality retrieval of Chroma documentation.\"\n",
    "    return {\"fa_summary\": fa_summary, \"processed_logs\": [f\"failure-analysis-on-log-{failure['id']}\" for failure in failures]}\n",
    "\n",
    "fa_builder = StateGraph(state_schema=FailureAnalysisState,output_schema=FailureAnalysisOutputState)\n",
    "fa_builder.add_node(\"get_failures\", get_failures)\n",
    "fa_builder.add_node(\"generate_summary\", generate_summary)\n",
    "fa_builder.add_edge(START, \"get_failures\")\n",
    "fa_builder.add_edge(\"get_failures\", \"generate_summary\")\n",
    "fa_builder.add_edge(\"generate_summary\", END)\n",
    "\n",
    "graph = fa_builder.compile()\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0373aec-e04a-444e-90f0-5e145e518e87",
   "metadata": {},
   "source": [
    "Here is the question summarization sub-grap, which uses `QuestionSummarizationState`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287ef670-9007-4450-b9ff-badde64b24dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarization subgraph\n",
    "class QuestionSummarizationState(TypedDict):\n",
    "    cleaned_logs: List[Log]\n",
    "    qs_summary: str\n",
    "    report: str\n",
    "    processed_logs: List[str]\n",
    "\n",
    "class QuestionSummarizationOutputState(TypedDict):\n",
    "    report: str\n",
    "    processed_logs: List[str]\n",
    "\n",
    "def generate_summary(state):\n",
    "    cleaned_logs = state[\"cleaned_logs\"]\n",
    "    # Add fxn: summary = summarize(generate_summary)\n",
    "    summary = \"Questions focused on usage of ChatOllama and Chroma vector store.\"\n",
    "    return {\"qs_summary\": summary, \"processed_logs\": [f\"summary-on-log-{log['id']}\" for log in cleaned_logs]}\n",
    "\n",
    "def send_to_slack(state):\n",
    "    qs_summary = state[\"qs_summary\"]\n",
    "    # Add fxn: report = report_generation(qs_summary)\n",
    "    report = \"foo bar baz\"\n",
    "    return {\"report\": report}\n",
    "\n",
    "qs_builder = StateGraph(QuestionSummarizationState,output_schema=QuestionSummarizationOutputState)\n",
    "qs_builder.add_node(\"generate_summary\", generate_summary)\n",
    "qs_builder.add_node(\"send_to_slack\", send_to_slack)\n",
    "qs_builder.add_edge(START, \"generate_summary\")\n",
    "qs_builder.add_edge(\"generate_summary\", \"send_to_slack\")\n",
    "qs_builder.add_edge(\"send_to_slack\", END)\n",
    "\n",
    "graph = qs_builder.compile()\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376aec2c-ee1a-436b-8a15-2bcbb3f778a4",
   "metadata": {},
   "source": [
    "## Adding sub graphs to our parent graph\n",
    "\n",
    "Now, we can bring it all together.\n",
    "\n",
    "We create our parent graph with `EntryGraphState`. \n",
    "\n",
    "And we add our sub-graphs as nodes! \n",
    "\n",
    "```\n",
    "entry_builder.add_node(\"question_summarization\", qs_builder.compile())\n",
    "entry_builder.add_node(\"failure_analysis\", fa_builder.compile())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9502eb4d-96fd-41bb-a6c5-2ecab7220eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entry Graph\n",
    "class EntryGraphState(TypedDict):\n",
    "    raw_logs: List[Log]\n",
    "    cleaned_logs: Annotated[List[Log], add] # This will be USED BY in BOTH sub-graphs\n",
    "    fa_summary: str # This will only be generated in the FA sub-graph\n",
    "    report: str # This will only be generated in the QS sub-graph\n",
    "    processed_logs:  Annotated[List[int], add] # This will be generated in BOTH sub-graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e584bf5f-111b-47a0-9565-1ccf0432662c",
   "metadata": {},
   "source": [
    "But, why does `cleaned_logs` have a reducer if it only goes *into* each sub-graph as an input? It is not modified.\n",
    "\n",
    "```\n",
    "cleaned_logs: Annotated[List[Log], add] # This will be USED BY in BOTH sub-graphs\n",
    "```\n",
    "\n",
    "This is because the output state of the subgraphs will contain **all keys**, even if they are unmodified. \n",
    "\n",
    "The sub-graphs are run in parallel.\n",
    "\n",
    "Because the parallel sub-graphs return the same key, it needs to have a reducer like `operator.add` to combine the incoming values from each sub-graph.\n",
    "\n",
    "But, we can work around this by using another concept we talked about before.\n",
    "\n",
    "We can simply create an output state schema for each sub-graph and ensure that the output state schema contains different keys to publish as output.\n",
    "\n",
    "We don't actually need each sub-graph to output `cleaned_logs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7e9f3b-ea5a-4b47-8a84-586848b7fa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entry Graph\n",
    "class EntryGraphState(TypedDict):\n",
    "    raw_logs: List[Log]\n",
    "    cleaned_logs: List[Log]\n",
    "    fa_summary: str # This will only be generated in the FA sub-graph\n",
    "    report: str # This will only be generated in the QS sub-graph\n",
    "    processed_logs:  Annotated[List[int], add] # This will be generated in BOTH sub-graphs\n",
    "\n",
    "def clean_logs(state):\n",
    "    # Get logs\n",
    "    raw_logs = state[\"raw_logs\"]\n",
    "    # Data cleaning raw_logs -> docs \n",
    "    cleaned_logs = raw_logs\n",
    "    return {\"cleaned_logs\": cleaned_logs}\n",
    "\n",
    "entry_builder = StateGraph(EntryGraphState)\n",
    "entry_builder.add_node(\"clean_logs\", clean_logs)\n",
    "entry_builder.add_node(\"question_summarization\", qs_builder.compile())\n",
    "entry_builder.add_node(\"failure_analysis\", fa_builder.compile())\n",
    "\n",
    "entry_builder.add_edge(START, \"clean_logs\")\n",
    "entry_builder.add_edge(\"clean_logs\", \"failure_analysis\")\n",
    "entry_builder.add_edge(\"clean_logs\", \"question_summarization\")\n",
    "entry_builder.add_edge(\"failure_analysis\", END)\n",
    "entry_builder.add_edge(\"question_summarization\", END)\n",
    "\n",
    "graph = entry_builder.compile()\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Setting xray to 1 will show the internal structure of the nested graph\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e86425-df94-4c35-a227-6fc3c196f859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy logs\n",
    "question_answer = Log(\n",
    "    id=\"1\",\n",
    "    question=\"How can I import ChatOllama?\",\n",
    "    answer=\"To import ChatOllama, use: 'from langchain_community.chat_models import ChatOllama.'\",\n",
    ")\n",
    "\n",
    "question_answer_feedback = Log(\n",
    "    id=\"2\",\n",
    "    question=\"How can I use Chroma vector store?\",\n",
    "    answer=\"To use Chroma, define: rag_chain = create_retrieval_chain(retriever, question_answer_chain).\",\n",
    "    grade=0,\n",
    "    grader=\"Document Relevance Recall\",\n",
    "    feedback=\"The retrieved documents discuss vector stores in general, but not Chroma specifically\",\n",
    ")\n",
    "\n",
    "raw_logs = [question_answer,question_answer_feedback]\n",
    "graph.invoke({\"raw_logs\": raw_logs})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472d6fad-9ddc-4bb7-95b8-56f0e59ae608",
   "metadata": {},
   "source": [
    "# Map-reduce\n",
    "\n",
    "## Review\n",
    "\n",
    "We're building up to a multi-agent research assistant that ties together all of the modules from this course.\n",
    "\n",
    "To build this multi-agent assistant, we've been introducing a few LangGraph controllability topics.\n",
    "\n",
    "We just covered parallelization and sub-graphs.\n",
    "\n",
    "## Goals\n",
    "\n",
    "Now, we're going to cover [map reduce](https://langchain-ai.github.io/langgraph/how-tos/map-reduce/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48efec28-4274-4d71-af67-2b1ead89282a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7328ba69-ba92-4e38-b333-ecc0081a1cb2",
   "metadata": {},
   "source": [
    "We'll use [LangSmith](https://docs.smith.langchain.com/) for [tracing](https://docs.smith.langchain.com/concepts/tracing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0151373-796c-403b-9fc4-adf4c0c1aef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_set_env(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langchain-academy\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253d8dd9-797c-4868-8f94-db296b4875ce",
   "metadata": {},
   "source": [
    "## Problem\n",
    "\n",
    "Map-reduce operations are essential for efficient task decomposition and parallel processing. \n",
    "\n",
    "It has two phases:\n",
    "\n",
    "(1) `Map` - Break a task into smaller sub-tasks, processing each sub-task in parallel.\n",
    "\n",
    "(2) `Reduce` - Aggregate the results across all of the completed, parallelized sub-tasks.\n",
    "\n",
    "Let's design a system that will do two things:\n",
    "\n",
    "(1) `Map` - Create a set of jokes about a topic.\n",
    "\n",
    "(2) `Reduce` - Pick the best joke from the list.\n",
    "\n",
    "We'll use an LLM to do the job generation and selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36db7ba3-6384-4d55-ba1e-26dd217ae226",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Prompts we will use\n",
    "subjects_prompt = \"\"\"Generate a list of 3 sub-topics that are all related to this overall topic: {topic}.\"\"\"\n",
    "joke_prompt = \"\"\"Generate a joke about {subject}\"\"\"\n",
    "best_joke_prompt = \"\"\"Below are a bunch of jokes about {topic}. Select the best one! Return the ID of the best one, starting 0 as the ID for the first joke. Jokes: \\n\\n  {jokes}\"\"\"\n",
    "\n",
    "# LLM\n",
    "model = ChatGroq(model=\"llama-3.3-70b-versatile\", temperature=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4303606-730a-4ff9-938c-9c26197aee5d",
   "metadata": {},
   "source": [
    "## State\n",
    "\n",
    "### Parallelizing joke generation\n",
    "\n",
    "First, let's define the entry point of the graph that will:\n",
    "\n",
    "* Take a user input topic\n",
    "* Produce a list of joke topics from it\n",
    "* Send each joke topic to our above joke generation node\n",
    "\n",
    "Our state has a `jokes` key, which will accumulate jokes from parallelized joke generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fd1d88-0800-4524-be92-d02a223231d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Subjects(BaseModel):\n",
    "    subjects: list[str]\n",
    "\n",
    "class BestJoke(BaseModel):\n",
    "    id: int\n",
    "    \n",
    "class OverallState(TypedDict):\n",
    "    topic: str\n",
    "    subjects: list\n",
    "    jokes: Annotated[list, operator.add]\n",
    "    best_selected_joke: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce7fdb3-530e-4a70-b608-dc632bd5d6f4",
   "metadata": {},
   "source": [
    "Generate subjects for jokes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1036b9c-9bb0-4652-a91e-e8e9140a4d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_topics(state: OverallState):\n",
    "    prompt = subjects_prompt.format(topic=state[\"topic\"])\n",
    "    response = model.with_structured_output(Subjects).invoke(prompt)\n",
    "    return {\"subjects\": response.subjects}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a8ab27-1ff5-4cc2-9e6c-22aa0057e7cc",
   "metadata": {},
   "source": [
    "Here is the magic: we use the [Send](https://langchain-ai.github.io/langgraph/concepts/low_level/#send) to create a joke for each subject.\n",
    "\n",
    "This is very useful! It can automatically parallelize joke generation for any number of subjects.\n",
    "\n",
    "* `generate_joke`: the name of the node in the graph\n",
    "* `{\"subject\": s`}: the state to send\n",
    "\n",
    "`Send` allow you to pass any state that you want to `generate_joke`! It does not have to align with `OverallState`.\n",
    "\n",
    "In this case, `generate_joke` is using its own internal state, and we can populate this via `Send`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601e1504-6e02-422c-b522-ed1e84fd5a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import Send\n",
    "def continue_to_jokes(state: OverallState):\n",
    "    return [Send(\"generate_joke\", {\"subject\": s}) for s in state[\"subjects\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadc7da5-9b2c-48d3-b42d-05dcdc471d91",
   "metadata": {},
   "source": [
    "### Joke generation (map)\n",
    "\n",
    "Now, we just define a node that will create our jokes, `generate_joke`!\n",
    "\n",
    "We write them back out to `jokes` in `OverallState`! \n",
    "\n",
    "This key has a reducer that will combine lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbe38fe-f886-4dff-be9b-8901b3197c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JokeState(TypedDict):\n",
    "    subject: str\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    joke: str\n",
    "\n",
    "def generate_joke(state: JokeState):\n",
    "    prompt = joke_prompt.format(subject=state[\"subject\"])\n",
    "    response = model.with_structured_output(Joke).invoke(prompt)\n",
    "    return {\"jokes\": [response.joke]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7428d281-4185-49fb-8b3a-9342c5814e85",
   "metadata": {},
   "source": [
    "### Best joke selection (reduce)\n",
    "\n",
    "Now, we add logic to pick the best joke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46098ef2-54e9-4a0d-bdbb-8e07c8219e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_joke(state: OverallState):\n",
    "    jokes = \"\\n\\n\".join(state[\"jokes\"])\n",
    "    prompt = best_joke_prompt.format(topic=state[\"topic\"], jokes=jokes)\n",
    "    response = model.with_structured_output(BestJoke).invoke(prompt)\n",
    "    return {\"best_selected_joke\": state[\"jokes\"][response.id]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b2188f-22a5-4500-98b6-d5c84a6e1bae",
   "metadata": {},
   "source": [
    "## Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f522a48a-1e82-44eb-ac8a-22b2280f2ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "# Construct the graph: here we put everything together to construct our graph\n",
    "graph = StateGraph(OverallState)\n",
    "graph.add_node(\"generate_topics\", generate_topics)\n",
    "graph.add_node(\"generate_joke\", generate_joke)\n",
    "graph.add_node(\"best_joke\", best_joke)\n",
    "graph.add_edge(START, \"generate_topics\")\n",
    "graph.add_conditional_edges(\"generate_topics\", continue_to_jokes, [\"generate_joke\"])\n",
    "graph.add_edge(\"generate_joke\", \"best_joke\")\n",
    "graph.add_edge(\"best_joke\", END)\n",
    "\n",
    "# Compile the graph\n",
    "app = graph.compile()\n",
    "Image(app.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5221c943-2371-447e-ac3f-1df89d98aa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the graph: here we call it to generate a list of jokes\n",
    "for s in app.stream({\"topic\": \"animals\"}):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4de2d19-6165-4806-97dd-bd7f54fe77a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
